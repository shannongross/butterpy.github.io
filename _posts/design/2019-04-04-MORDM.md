---
layout: page
sidebar: left
subheadline: Single and multivariate linear regression
title:  "An Easy Introduction to Multi-Objective Robust Decision Making"
teaser: "An straightforward introduction to Multi-Objective Robust Decision Making in Python using EMA workbench."
breadcrumb: true
tags:
    - multiobjective evolutionary algorithms
    - robust decision making
    - exploratory modeling
    - policy analysis with python
    - uncertainty analysis
categories:
    - modeling
header:
    image: linear.webp
    background-color: "#fabb00"
    caption: This is a caption for the header image with link
    caption_url: https://unsplash.com/
show_meta: true
---

MORDM is an exploratory modelling approach that combines advanced optimization methods with robustness metrics in order to find solutions to problems that have multiple objectives. In the following article, the process is broken down into four easy-to-follow steps, including links and examples in python to follow along.

<div class="panel radius" markdown="1">
**Table of Contents**
{: #toc }
*  TOC
{:toc}
</div>

# What is Multi-Objective Robust Decision-Making (MORDM)?

The general task of MORDM is to come up with a promising strategy to tackle a policy problem, and then to stress-test and refine that policy until it is "robust".

The relationship between policy levers and performance outcomes depends on many uncertain factors beyond of the control of the policymaker. Traditionally, this uncertainty is accounted for by using an expected value from on an established probability distribution. However, this is difficult in situations of deep uncertainty, when stakeholders do not agree on the likelihood or impact of a particular uncertainty on the system. An alternative is to test policy performance against a wide range of values for these unknown parameters, thus stakeholders do not need to agree on the probabilities of these parameters upfront. That is one of the major advantages of the Many-Objective Decision Making (MORDM) approach.


MORDM combines two powerful concepts:
1. Robust Decision Making (RDM)
2. Many Objective Evolutionary Optimization (MOEA)

For a useful primer on the method, see the [waterprogramming blog](https://waterprogramming.wordpress.com/2013/04/16/robust-decision-making-rdm-concepts-and-methods/) article by Kasprzyk, the original author, which contains a detailed breakdown.

- This thesis adopts the MORDM method proposed by Kasprzyk et al. (2013), which uses a many-objective evolutionary algorithm (MOEA) to find a set of policies close to the Pareto-optimal front.

## Why use it?
There are many different ways to computationally search for promising candidate policies in light of conflicting objectives. MORDM is one strategy for finding solutions to complex problems that are approximately **Pareto-optimal**. The notion of Pareto optimality means finding a set of multiple best solutions for the problem, where each solution is a compromise between different objectives. A clear estimate of the Pareto front can be highly useful in helping policymakers to select robust plans as well as to learn about the underlying dynamics of the system.

MORDM is intended to be done in collaboration with decision makers, in order to incorporate lessons learned as part of the policy deliberation process (Kasprzyk et al., 2013). Since MORDM does not require assumptions about policymaker preferences before determining the Pareto front, this approach represents *a posteriori* decision support.



![a posteriori decision analysis]({{site.baseurl}}/images/aposteriori.jpg)

Decision preferences may be incorporated into a quantitative model upfront (*a priori*) or after computing all possible solutions (*a posteriori*). The latter is particularly useful when decision makers do not know exactly what they want until they see potential solutions, such as when eating at a buffet.

Adopting an *a posteriori* approach has the benefit of enabling the decision maker to gain an understanding of a larger set of superior solutions without the premature aggregation of performance measures. Ultimately, however, it is up to the decision maker to select the final policy once these trade-offs between different options are made clear.



## MORDM Steps
Experimentation using MORDM contains four main steps: (1) formulating the problem statement in a way that the objectives can be evaluated quantitatively according to the needs of the decision maker; (2) generating a Pareto approximate set of initially promising strategies; (3) stress-testing the promising set of strategies by exploring their performance under a wide range of plausible futures; and (4) adjusting the most robust candidate strategies to guard against any vulnerabilities identified in scenario discovery. Optionally, the last step can be fed back into the beginning to incorporate decision-maker feedback and lessons learned.


![MORDM search steps]({{site.baseurl}}/images/mordm-img.jpg)


### Step 1. Formulate the Problem
To formulate a problem in terms of (multi-objective) robust decision-making, it is highly useful to use the [XLRM framework](www.google.com). An example of the XLRM framework, which structures the problem into its exogeneous uncertainties (X), policy levers (L), internal relationships (R) and outcomes (M) is shown below.

![XLRM example]({{site.baseurl}}/images/xlrm-example1.jpg)

Often, there is difficulty trying to define exactly which exogenous uncertainties to include (and how). As explained in the waterprogramming blog, some useful guidelines are to:
- Consider variables that are *projected into the future*. Is there another way this future change could be modeled?
- Ask if there are hidden *assumptions* in the model that other stakeholders may disagree with. In this case, it may be better to model such assumptions as X, so that different values can be tested.
- Determine if there variables that are not relevant now, but could *become* relevant later.

Asking yourself these questions can help to identify those uncertainties that are ideally suited to systematic exploration.

### Step 2. Search for Promising Options
Now that you have formulated the problem, you may have a few options in mind of how it could be handled. For instance, in the example **above**, you might think to: x, y, z. As explained previously, coming up with strategies like this is known as *a priori* analysis, because you're coming up with them upfront (using your own gut instinct). However, the problem with creating options *a priori* is that your critics are likely to claim that they are biased or wrong. A different method is to create these options computationally, which is what is done in MORDM. But how can a computer tell you what a "good" option is? Enter: MOEAs.

pre-specified policies are no longer used. Instead, policies are generated with the assistance of a many-objective evolutionary algorithm (MOEA) known as NSGA-II to search for potentially high-performing combinations of policy levers. Rather than only studying pre-specified policy options, the directed search phase of MORDM aims to find promising new combinations of policy levers that perhaps would not have been thought of without computational assistance.
The aim of this section is to use the NSGA-II algorithm to approximate the Pareto front for each problem formulation. The operators used in evolutionary algorithms are used to help find solutions close to the Pareto front, though due to the stochasticity in the algorithm this is not guaranteed. It is expected that if bad individuals are created, they will be eliminated by the selection operator and good individuals will be emphasized (Deb, 2001). In order to ensure high-quality results, the experiments should be parameterized carefully.

#### USING MANY-OBJECTIVE EVOLUTIONARY ALGORITHMS
MORDM incorporates MOEAs within the search phase in order to perform global optimization. This enables MORDM to discover high-performing policy options even when the problem is complex. Mimicking natural processes of evolution, MOEAs iteratively evaluate possible strategies across the many objectives until the best candidates are found. Compared to classical optimization methods, MOEAs present the following advantages:
- Rather than handling a single solution, the population-based approach of MOEAs can be used to find a large number of solutions in a single run.
- MOEAs are ideally suited to working in parallel systems, which can dramatically reduce computational expenses.
- Because the algorithm’s evolutionary processes are separate from the issue it is applied to, MOEAs are easily applied to different problems.

The application of MOEAs to many-objective policy problems is useful for keeping performance measure disaggregated while enabling the evaluation of trade-offs between various alternatives (Kasprzyk et al., 2013). In short, MOEAs provide an efficient way to determine the Pareto front and highlight potentially robust policy options.

### Step 3. Uncertainty Analysis
The Uncertainty Analysis step is all about stress-testing the options you found in Step 2. While your options from the previous step seem promising, you only analyzed them under one "state of the world". A state of the world, or *uncertainty ensemble*, is one possible set of values that your uncertain parameters could take. For instance, common states of the world are:
- The "best-case scenario": This would be when your wedding day has the most perfect weather and all of your guests are having an incredible time. You *hope* this is the future that comes to pass, but if you only make plans counting on this to happen then your plans will not be very resilient.
- The "best-guess scenario": When you assign each of your uncertain variables with their most likely value. Based on your intuition about the system, this is how you think the future will *probably* unfold (so of course you want a strategy that performs well in this case).
- The "worst-case scenario": This would be the case when every one of your uncertain parameters takes on their worst possible value. In the example of your wedding day, that would be weather="Thunderstorm"; guests_dancing=0; drinks_available=0. You don't think that all of these terrible things will actually happen, but just in case maybe its a good idea to have a rain tent prepared; to vet the DJ with some of your friends; and to order a few extra cases of wine.
  - The old adage, "expect the worst, hope for the best" may be appropriate here, however bear in mind that you can't always afford the most conservative strategy to guard against the worst-case scenario. If your wedding is on a tight budget for instance, you'll have to pick between ordering the better DJ *or* the extra cases of wine.
- And many other "in-between scenarios". On your wedding day, you may run out of wine but have beautiful weather. The guests may love the DJ you picked but get tired of the large number of speeches given. How can you guard against these uncertainties if it seems like there's an infinite number of scenarios you have to be prepared for?

This is where uncertainty analysis using ensembles comes in. Each ensemble is a different plausible future state of the world, using

In this section, the non-dominated sets of promising policy options from the directed search were subjected to computational exploration by systematically varying the uncertain parameters. By testing the policy options against a wider range of uncertainties (beyond the initially-specified reference scenario), the performance of policies under a much broader uncertainty ensemble is provided. Specifically, the performance of candidate strategies was re-evaluated under a computationally generated 300-point uncertainty ensemble, leading to sets of strategies that are not only approximately “optimal” but also robust against many different plausible futures. The benefit of this process is that it can help decision makers realize which strategies are most sensitive to deviations from the reference scenario.

Running this ensemble against the promising policy results allows the implications of a wide range of combinations of these uncertain values to be explored. It is useful to create this scenario ensemble upfront, so that each problem formulation can be tested against the same set of uncertainty combinations.
Once the ensemble of 300 scenarios is generated, the promising candidates from step 2 are subjected to experimentation in order to see which ones are still high-performing. The interventions that still perform well against a wide range of plausible future states of the world are called “robust.” The robustness of each candidate strategy is calculated based on its performance against an outcome indicator (a robustness metric),

### Step 4. Scenario Discovery

“Under what plausible future states of the world are the robust policy options vulnerable?” These important scenarios (which can be difficult to arrive at without computational assistance) are determined through the process of scenario discovery. As described in the Methodology section, a statistical bump-hunting algorithm known as PRIM is used to identify regions of the large, multi-dimensional uncertainty space where the robust policies are vulnerable to failure. Here, the definition of policy vulnerability is adapted from (Herman et al., 2015) to mean the ranges of deeply uncertain parameters observed to cause policy performance to degrade below a set threshold.

DESCRIBING POOR PERFORMANCE
How badly does a policy have to perform before it is declared to be a total failure? For many public health problems, it is difficult to quantify a number that demarcates between “failure” and “not failure.” However, having a performance threshold is useful for scenario discovery when looking for conditions that cause strategies to perform badly. Some policy issues may have a natural boundary or well-characterized stakeholder preferences that make setting this threshold obvious; while other issues may be less well-defined as to what performance value qualifies as a failure (Bryant & Lempert, 2010). Public health issues like the current case study are generally of the kind without a clear-cut threshold (since some stakeholders might say that it is unacceptable to have even one life lost).

This step used scenario discovery to find important combinations of uncertainties that decision makers should be aware of for policy design.
Ultimately, scenario discovery is a useful tool for learning and for improving the performance of the robust candidate strategies.


## Tips for doing MORDM w EMA workbench

- Optimization - observes the state of the lake and releases the amount of pollution at each time step. Normally, each time step is an opportunity to make a decision.
- Epsilons: the grid we use in the objective space
- Epsilon progress - as long as we jump to a new quadrant it counts as progress. Doesn’t move into new box, doesn’t count as progress
- Hypervolume - the sum of the boxes. Hypervolume is the most useful for convergence, but it is the most computationally expensive, particularly with more than 4-5 objectives. WIll take a very long time more than that.
- Optimization will take a long time - do it overnight, use the multiprocessor
- Cython - translates python code into C and back to speed up your code. Can use for optimizing the lake model - way faster.
- Prim (scenario discovery) - tradeoff curve generated by prim. Patient hill climbing algorithm. Each step in the optimization trajectory is kept, then we look at each step (the path that the hill climbing took) coverage is how many are contained in the box. Density of those contained in the box, how many are of interest? Over time, make the box smaller (by cutting off the stuff that doesn’t matter) look at the box definition of different boxes. Select a box in the tradeoff curve. Select a point that balances coverage and density.

## Decision Analysis
- The aim of decision analysis is to foster learning, rather than to dictate choice. As a policy analyst, it is not our job to make the decision, but instead to help support the negotiation space.
- The role of open exploration: investigative mapping from uncertainties to outcomes. Identifying differences that make a difference. Iterative stress testing to reduce vulnerabilities.
- Results suggest that including robustness as a decision criterion can dramatically change the formulation of complex environmental management problems as well as the negotiated selection of candidate alternatives to implement.
- The goal is to find a set of Pareto optimal strategies that are robust against the various uncertainties and to understand tradeoffs between objectives.


## Closing Remarks - MORDM
- Though to the author’s knowledge MORDM has not been used so far to inform public health policymaking, the approach holds great potential for supporting experts in discovering, analyzing, and fine-tuning health development policies.


## Other Modeling Posts
{: .t60 }
{% include list-posts tag='modeling' %}



<!-- hello

| RDM | MORDM     |
| :------------- | :------------- |
| iterative stress testing using scenario discovery. Start with some pre-specified decision scenarios, figure out how these pre-specified alternatives fail or succeed. Presupposes that you have decision alternatives that you can start from | use to find alternatives with which you can start from Worst case discovery, once you have a strategy you like, you can run this to determine what your worst case is going to be given this strategy is implemented   |


## The problem with "optimality"
> Cognitive myopia arises when decision makers inadvertently ignore aspects of the problem (such as important decision alternatives or key planning objectives) by focusing on a limited number of a priori specified alternatives or a narrowly defined, highly aggregated definition of “optimality”

goodbye-->
